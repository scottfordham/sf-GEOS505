{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80579d38",
   "metadata": {},
   "source": [
    "## 06.1 Web Scraping Geospatial Data\n",
    "\n",
    "In this notebook, we will examine how we can scrape metadata from the web to transform it into geospatial data. The web contains all kinds of geospatial data, if you know where to look for it. For example, headers on Twitter/X posts sometimes contain the geographic location from which they were sent (you can and should turn this feature off, but it's useful), photos sometimes have information from the GPS chip in the camera or phone where the picture was taken (again, you can turn this off if you share your photos), and your Strava posts certainly can be mined for geographic data (if you're interested in how this can be __very__ bad for OPSEC in \"certain careers\", see [this article](https://www.bellingcat.com/resources/articles/2018/07/08/strava-polar-revealing-homes-soldiers-spies/)).\n",
    "\n",
    "Here we won't be doing anything nefarious, but will be addressing a minor annoyance in the serving of lidar point cloud data. For the past decade or so, the USGS has sought to perform lidar surveys over the entire US to provide updated topographic information via the 3D Elevation Program (3dep, [https://www.usgs.gov/3d-elevation-program](https://www.usgs.gov/3d-elevation-program)). Individual surveys are performed in support of a variety of agencies, and the data processed and served by the USGS in the form of derived data products like gridded digital elevation models (DEMs) and canopy height models (CHMs). However, they also preserve the raw point cloud data as compressed las files (along with metadata for those files), which are potentially useful for analyzing structural characteristics of forests in the UBRB. However, they aren't served in a particularly helpful format. Here is a link to the raw project data: [https://rockyweb.usgs.gov/vdelivery/Datasets/Staged/Elevation/LPC/Projects/ID_FEMAHQ_2018_D18/ID_FEMAHQ_2018/](https://rockyweb.usgs.gov/vdelivery/Datasets/Staged/Elevation/LPC/Projects/ID_FEMAHQ_2018_D18/ID_FEMAHQ_2018/). \n",
    "\n",
    "In the metadata folder are something like 10,000 XML files that contain metadata on each of the approximately 1 km${}^2$ point cloud tiles. In the LAZ folder, meanwhile, is the link to the LAS data for each of those tiles. The problem? We don't have a way of knowing where each of those LAS files is located on earth, except to click through the XML file and look at the bounds of each tile. Instead, we'll use some web sleuthing and scraping to read every one of those XML files, take note of the tile ID and the las file download URL, and we'll assemble a geospatial data file (a GeoDataFrame, specifically) that we can then examine on an interactive map. This will allow us to zoom around the UBRB and identify the specific tiles that we need and download them directly. We could also use our trusty GeoPandas skills to identify only those tiles that fall within a study area (like a watershed), use the `.clip()` operator to get that information, and then export the table of download URLs for LAS files in our study area. Below we do the first and most time-consuming step: reading all of the XML files, getting the bounding coordinates of each lidar tile, and assembling them into a GeoDataFrame.\n",
    "\n",
    "### 1. Imports and Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bf2a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # Needed to open a web site\n",
    "from bs4 import BeautifulSoup # Needed to parse an HTML website\n",
    "from pyproj import CRS # Needed to define thee CRS of our output data file\n",
    "from shapely.geometry import box # Needed to create features for our geodataframe\n",
    "import geopandas as gpd # Needed to create our geospatial data layer\n",
    "\n",
    "# Define a potential list of projections for the project lidar data\n",
    "PROJ_NAME_MAP = {\n",
    "    'NAD83 / Conus Albers': ('aea', 'NAD83'),\n",
    "    'Albers Conical Equal Area': ('aea', None),\n",
    "    'Lambert Conformal Conic': ('lcc', None),\n",
    "    'Transverse Mercator': ('tmerc', None),\n",
    "    # extend as you encounter new projection names\n",
    "}\n",
    "\n",
    "# Base URL for the project – this could be changed for other projects\n",
    "gs_3dep_url = 'https://rockyweb.usgs.gov/vdelivery/Datasets/Staged/Elevation/LPC/Projects/ID_FEMAHQ_2018_D18/ID_FEMAHQ_2018/'\n",
    "\n",
    "# XML Metadata files would be here\n",
    "gs_3dep_meta_url = 'metadata/'\n",
    "\n",
    "# JPG browse images would be here\n",
    "gs_3dep_browse_url = 'browse/'\n",
    "\n",
    "# Raw LAS files would be here\n",
    "gs_3dep_las_url = 'LAZ/'\n",
    "\n",
    "output_geojson_geo = 'ID_FEMAHQ_2018_las_tiles.geojson' # Output geospatial data layer in geographic projection\n",
    "output_geojson_proj = 'ID_FEMAHQ_2018_las_tiles_alb.geojson' # Output geospatial data layer in albers projection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c85051c",
   "metadata": {},
   "source": [
    "### 2. Function Definitions:\n",
    "\n",
    "This first function will take the input URL where all of the XML metadata files can be found and return a list of links to those XML files.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43139da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html(gs_3dep_url):\n",
    "    \n",
    "    reqs = requests.get(gs_3dep_url) # Open the URL\n",
    "    soup = BeautifulSoup(reqs.text, 'html.parser') # Create a structure to parse\n",
    "\n",
    "    xml_links = [] # Empty list to which we will append XML file links\n",
    "    for link_tag in soup.find_all('a', href=True): # Find all <a> tags with an href attribute\n",
    "        href = link_tag.get('href')\n",
    "        if href and href.lower().endswith('.xml'): # Check if the link ends with '.xml'\n",
    "            # You may need to construct absolute URLs if hrefs are relative\n",
    "            if href.startswith('http') or href.startswith('https'):\n",
    "                xml_links.append(href)\n",
    "            else:\n",
    "                # Handle relative URLs (basic example, might need more robust handling for complex cases)\n",
    "                from urllib.parse import urljoin\n",
    "                absolute_url = urljoin(gs_3dep_url, href)\n",
    "                xml_links.append(absolute_url)\n",
    "                \n",
    "    return xml_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347861fd",
   "metadata": {},
   "source": [
    "This second function take an individual URL to an XML metadata file for a 1 km${}^2$ lidar tile, opens the XML file, and parses that XML file to get the bounding coordinates of that lidar file. It also finds projection information from the metadata file. It returns a dictionary containing the bounding box of the lidar tile, and the CRS of the tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "955872ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tile_xml(url: str) -> dict:\n",
    "    resp = requests.get(url, timeout=60)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.content, 'xml')\n",
    "\n",
    "    bounds_xml = soup.find('spdom').find('bounding')\n",
    "    bbox = {\n",
    "        'west': float(bounds_xml.find('westbc').text),\n",
    "        'east': float(bounds_xml.find('eastbc').text),\n",
    "        'north': float(bounds_xml.find('northbc').text),\n",
    "        'south': float(bounds_xml.find('southbc').text),\n",
    "    }\n",
    "\n",
    "    mapproj = soup.find('horizsys').find('planar').find('mapproj')\n",
    "    proj_name = mapproj.find('mapprojn').text.strip()\n",
    "\n",
    "    # prefer EPSG or WKT if present\n",
    "    epsg_tag = soup.find('refsysid')\n",
    "    if epsg_tag and epsg_tag.find('code'):\n",
    "        crs = CRS.from_epsg(int(epsg_tag.find('code').text))\n",
    "    elif mapproj.find('planarco'):\n",
    "        crs = CRS.from_wkt(mapproj.find('planarco').text)\n",
    "    else:\n",
    "        proj_alias, datum = PROJ_NAME_MAP.get(proj_name, (None, None))\n",
    "        params = {}\n",
    "        if proj_alias == 'aea' and mapproj.albers:\n",
    "            stdpars = mapproj.albers.find_all('stdparll')\n",
    "            params = {\n",
    "                'proj': 'aea',\n",
    "                'lat_1': float(stdpars[0].text),\n",
    "                'lat_2': float(stdpars[1].text),\n",
    "                'lat_0': float(mapproj.albers.find('latprjo').text),\n",
    "                'lon_0': float(mapproj.albers.find('longcm').text),\n",
    "                'x_0': float(mapproj.albers.find('feast').text),\n",
    "                'y_0': float(mapproj.albers.find('fnorth').text),\n",
    "                'datum': datum or 'WGS84',\n",
    "                'units': 'm',\n",
    "            }\n",
    "        # add elif blocks for other projections (lcc, tmerc, utm...) using their tags\n",
    "        if params:\n",
    "            crs = CRS.from_dict(params)\n",
    "        else:\n",
    "            crs = CRS.from_user_input(proj_name)  # final fallback\n",
    "\n",
    "    return {'bbox': bbox, 'crs': crs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c08346a",
   "metadata": {},
   "source": [
    "### 3. Create the Geospatial Data Layer\n",
    "\n",
    "Now we will call the functions above to create our data layer. First, parse the metadata HTML site to get links to the individual XML files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a5530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the URLs for all of the XML files\n",
    "project_xmls = parse_html(gs_3dep_url+gs_3dep_meta_url) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75a2785",
   "metadata": {},
   "source": [
    "Now, loop through that list of XML files and parse them for bounding boxes. Append these to a list that we will use to create a GeoDataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ed53ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 99.99% of XML files...\r"
     ]
    }
   ],
   "source": [
    "# Get the metadata for the first XML file, we'll use this to set the \n",
    "# CRS for all tiles in this project\n",
    "project_meta = parse_tile_xml(project_xmls[0])  \n",
    "project_crs = project_meta['crs']\n",
    "\n",
    "# Create an empty list to which we'll append the information for each XML file/lidar tile\n",
    "records = []\n",
    "for i, url in enumerate(project_xmls): # enumerate gives us both the individual url and an index\n",
    "    meta = parse_tile_xml(url) # Get the bounding box for this XML file\n",
    "    records.append(\n",
    "        {\n",
    "            'tile_id': url[-14:-4], # ID of the tile in something like wXXXXnXXXX format\n",
    "            'xml_url': url, # Link to the xml file\n",
    "            'browse_url': url.replace(gs_3dep_meta_url,gs_3dep_browse_url)[0:-4]+'.jpg', # Link to the JPG browse image\n",
    "            'laz_url': url.replace(gs_3dep_meta_url,gs_3dep_las_url)[0:-4]+'.laz', # Link to the raw, compressed las file\n",
    "            'geometry': box(meta['bbox']['west'], meta['bbox']['south'],\n",
    "                            meta['bbox']['east'], meta['bbox']['north']), # Geometry of this feature\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Print progress\n",
    "    print(f'Completed {i/len(project_xmls):.2%} of XML files...', end='\\r', flush=True)\n",
    "\n",
    "# Create the GeoDataFrame from the XML records\n",
    "tiles_gdf_geo = gpd.GeoDataFrame(records, crs='EPSG:4326')\n",
    "\n",
    "# Create another version in the projection of the lidar files themselves\n",
    "tiles_gdf_proj = tiles_gdf_geo.to_crs(project_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea59e01",
   "metadata": {},
   "source": [
    "### 4. Save Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a7ea065",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_gdf_geo.to_file(output_geojson_geo)\n",
    "tiles_gdf_proj.to_file(output_geojson_proj)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
